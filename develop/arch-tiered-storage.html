<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<title>WiredTiger: Tiered Storage</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="sorttable.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="wiredtiger.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><a href="http://wiredtiger.com/"><img alt="Logo" src="LogoFinal-header.png" alt="WiredTiger" /></a></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">Version 11.3.0</span>
   </div>
   <div id="projectbrief"><!-- 11.3.0 --></div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<div class="banner">
  <a href="https://github.com/wiredtiger/wiredtiger">Fork me on GitHub</a>
  <a class="last" href="http://groups.google.com/group/wiredtiger-users">Join my user group</a>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('arch-tiered-storage.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Tiered Storage </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><div class="arch_head"><table class="doxtable">
<tr>
<th rowspan="2" style="width:10%;"> <div><a href="arch-index.html"><img class="arch_thumbnail" src="wt_diagram.png" usemap="#wt_diagram_map" style="background-image: url(wt_diagram.png)"></a></div></th><th style="width:44%">Data Structures</th><th style="width:45%">Source Location</th></tr>
<map id="wt_diagram_map" name="wt_diagram_map">
<area shape="rect" id="id1" href="modules.html" title="modules.html" alt="" coords="248,128,283,144"/>
<area shape="rect" id="id2" href="arch-cache.html" title="arch-cache.html" alt="" coords="204,546,248,562"/>
<area shape="rect" id="id3" href="arch-cursor.html" title="arch-cursor.html" alt="" coords="234,224,280,240"/>
<area shape="rect" id="id4" href="arch-eviction.html" title="arch-eviction.html" alt="" coords="303,546,356,562"/>
<area shape="rect" id="id5" href="arch-logging.html" title="arch-logging.html" alt="" coords="388,650,443,666"/>
<area shape="rect" id="id6" href="arch-schema.html" title="arch-schema.html" alt="" coords="123,224,179,240"/>
<area shape="rect" id="id7" href="command_line.html" title="command_line.html" alt="" coords="374,23,430,39"/>
<area shape="rect" id="id8" href="arch-log-file.html" title="arch-log-file.html" alt="" coords="308,865,339,897"/>
<area shape="rect" id="id9" href="arch-metadata.html" title="arch-metadata.html" alt="" coords="25,328,89,344"/>
<area shape="rect" id="id10" href="arch-python.html" title="arch-python.html" alt="" coords="84,23,157,39"/>
<area shape="rect" id="id11" href="arch-snapshot.html" title="arch-snapshot.html" alt="" coords="297,441,371,457"/>
<area shape="rect" id="id12" href="arch-transaction.html" title="arch-transaction.html" alt="" coords="298,328,387,344"/>
<area shape="rect" id="id13" href="arch-hs.html" title="arch-hs.html" alt="" coords="93,642,140,674"/>
<area shape="rect" id="id14" href="arch-block.html" title="arch-block.html" alt="" coords="196,642,256,674"/>
<area shape="rect" id="id15" href="arch-dhandle.html" title="arch-dhandle.html" alt="" coords="144,320,205,352"/>
<area shape="rect" id="id16" href="arch-data-file.html" title="arch-data-file.html" alt="" coords="179,865,245,897"/>
<area shape="rect" id="id17" href="arch-row-column.html" title="arch-row-column.html" alt="" coords="117,433,205,465"/>
<area shape="rect" id="id18" href="arch-fs-os.html" title="arch-fs-os.html" alt="" coords="175,755,357,771"/>
</map>
<tr>
<td><code><code>WT_TIERED<br  />
WT_TIERED_TIERS</code></code></td><td><code><code>src/include/tiered.h<br  />
src/tiered/</code></code></td></tr>
</table>
</div><p> <b>Caution: the Architecture Guide is not updated in lockstep with the code base and is not necessarily correct or complete for any specific release.</b></p>
<dl class="section warning"><dt>Warning</dt><dd>Tiered storage is considered an experimental part of the WiredTiger library, and is not yet ready for production workloads.</dd></dl>
<h1><a class="anchor" id="ts_intro"></a>
Introduction and Definitions</h1>
<p>Tiered storage allows B-Trees to be stored into multiple places, more recently updated blocks on local disk and less recently updated blocks in cloud storage. The term <em>object</em> refers to one of these written btree parts, whether the object resides on the local disk, as an <em>object file</em>, or in the cloud, as a <em>cloud object</em>. The mechanism to create new objects is a <a class="el" href="struct_w_t___s_e_s_s_i_o_n.html#a6550c9079198955c5071583941c85bbf" title="Write a transactionally consistent snapshot of a database or set of individual objects.">WT_SESSION::checkpoint</a> API call with the <code>flush_tier</code> configuration set. For brevity, it is called a <code>flush_tier</code> call.</p>
<p>When in use, a tiered btree, like a non-tiered btree, may have some recently used or modified data that resides in memory pages. This in-memory representation is the same between a tiered btree and a non-tiered btree, it is only how data is stored on disk and in cloud objects that makes these btrees different.</p>
<p>Tiered storage is configured and enabled on the connection via <a class="el" href="group__wt.html#gacbe8d118f978f5bfc8ccb4c77c9e8813" title="Open a connection to a database.">wiredtiger_open</a> configuration options (e.g. <code>tiered_storage=(name="s3_store",bucket="...")</code> ). Once enabled on the connection, the configuration applies to all tables created. The configuration can be overridden on individual tables with configuration options for <a class="el" href="struct_w_t___s_e_s_s_i_o_n.html#a358ca4141d59c345f401c58501276bbb" title="Create a table, column group, index or file.">WT_SESSION::create</a> . The related <a class="el" href="struct_w_t___s_e_s_s_i_o_n.html#a358ca4141d59c345f401c58501276bbb" title="Create a table, column group, index or file.">WT_SESSION::create</a> configuration options allow the caller to specify a different storage provider or bucket name or to have the table opt out of tiered storage.</p>
<h1><a class="anchor" id="ts_objectid"></a>
Object IDs</h1>
<p>In response to a <code>flush_tier</code> call, WiredTiger creates a new object for each changed btree. Each new object created gets a new <em>objectid</em>, incremented from the previous objectid used with that btree. The new object lives initially as a regular local disk <em>object file</em>, and the disk file name includes the name of the table and the objectid. This makes it is easy to see from a directory listing which tables and which objects for that table are represented on disk. Once a new (<em>N+1</em>) object is created, all writes to the previous (<em>N</em>) object are completed and that <em>N</em> object becomes read-only, like all objects before it. At this point, the <em>N</em> object is queued to be copied to cloud storage. After that copy successfully completes, the local copy of <em>N</em> is queued for removal, see <a class="el" href="arch-tiered-storage.html#ts_block_local_file_removal">Local File Removal</a>.</p>
<h1><a class="anchor" id="ts_checkpoints"></a>
Checkpoints</h1>
<p>A normal, non-tiered table, can be thought of as an active or <em>live</em> btree, with zero or more checkpoints that are fully represented in the single disk object file. Each checkpoint has its own root page, and can be considered its own btree. Each of these btrees is a set of pages referencing each other as a tree, some in memory, some on disk. A tiered table is the same, having a <em>live</em> btree and a set of checkpoint btrees. However, both the live btree and the checkpoints may have pages that span multiple object files and/or cloud objects.</p>
<p>For tiered storage, each object, other than the current file object, is guaranteed to contain at least one checkpoint. When there is a switch to a new (<em>N+1</em>) current object, a checkpoint in the previous (<em>N</em>) file object is guaranteed, because a <a class="el" href="struct_w_t___s_e_s_s_i_o_n.html#a6550c9079198955c5071583941c85bbf" title="Write a transactionally consistent snapshot of a database or set of individual objects.">WT_SESSION::checkpoint</a> call with a <code>flush_tier</code> option is required to switch. A checkpoint in the <em>N</em> object refers to blocks in object <em>N</em> as well as previous (<em>N-1</em>, <em>N-2</em>, ...) objects.</p>
<h1><a class="anchor" id="ts_block_manager"></a>
Block Manager</h1>
<p>A btree is organized logically as a set of pages. Each page is either a leaf page, containing keys and values, or an internal page, which has references to subordinate leaf pages. Writing a page goes through the block manager, which has the job of locating a free position in the file. That position, along with the block size and a checksum of the content, is packaged up as an <em>address cookie</em> (a sequence of bytes) that is returned to the caller. Address cookies are further described in <a class="el" href="arch-block.html#block_what">What is a block?</a>.</p>
<h2><a class="anchor" id="ts_cookies"></a>
Cookies in Tiered Storage</h2>
<p>For tiered storage, a reference to a written location needs to indicate an objectid. If the reference is from the same object, no explicit objectid is needed, and the <code>(position, size, checksum)</code> triple can be used. Otherwise, a four-tuple <code>(position, size, checksum, objectid)</code> is used. Because the byte size of the cookie is given, and the objectid appears last, the code that interprets cookies can deduce which kind it is given. If the decoder has seen three entries and it has reached the end of its data, then it has a triple. If the decoder still has more data, then it also contains an objectid.</p>
<p>Object numbering starts at 1, so an objectid of 0 references the same object as the cookie's location. Because of these properties, a file from a non-tiered btree can be upgraded to the first object of a tiered btree without changing its contents. Every reference in the non-tiered btree is a triple, meaning there is an implied a zero objectid. Thus each reference must be to the same object.</p>
<h2><a class="anchor" id="ts_block_files"></a>
Files in the Block Manager</h2>
<p>The data structure that the block manager uses for accessing a btree's files or objects is a <code>WT_BM</code>. For non-tiered tables, the <code>WT_BM-&gt;block</code> field references a single <code>WT_BLOCK</code> <em>handle</em>, which represents the single file. This block handle exists as long as the btree is open, so no locking or coordination is needed to access it.</p>
<p>For tiered tables, multiple objects are associated with a <code>WT_BM</code>, and each object is referenced by a <code>WT_BLOCK</code>. The current writable object is referred to by <code>WT_BM-&gt;block</code>, and a list of objects recently referenced appears in the <code>WT_BM-&gt;handle_array</code>. The <code>handle_array</code> always includes the current object. When an existing tiered table is opened from disk, a new <code>WT_BM</code> is created. A <code>WT_BLOCK</code> for the current writable object is also created, and <code>WT_BM-&gt;block</code> is set to that and also added to the (otherwise empty) <code>handle_array</code>. As other objects are referenced, new <code>WT_BLOCK</code> handles are created to represent them, and they are added to the <code>handle_array</code>.</p>
<p>Block handles (<code>WT_BLOCK</code>) in the handle array are cached from the complete list of open block handles kept in the connection (<code>WT_CONNECTION_IMPL-&gt;block_hash</code>). Blocks in the connection hash table are reference counted. When the reference count goes to zero, the handle is removed and freed, and the underlying file handle is closed.</p>
<p>When a <code>flush_tier</code> is done, each table that had changes written as part of the checkpoint will have its <code>WT_BM::switch_object</code> function called. This function creates a new <code>WT_BLOCK</code> for the new <em>N+1</em> file object, and adds it to the <code>handle_array</code>. It does not yet point the <code>WT_BM-&gt;block</code> to the new block handle, as there may still be writes in progress to the old block. When all writes from the checkpoint have completed, the block manager sets <code>WT_BM-&gt;block</code> to point to the block handle for the new file object (i.e., file object <em>N+1</em>).</p>
<p>Since there are multiple threads accessing the block manager, a read/write lock on the <code>WT_BM</code> is used to access, modify or grow the array of <code>WT_BLOCK</code> handles.</p>
<h2><a class="anchor" id="ts_block_local_file_removal"></a>
Local File Removal</h2>
<p>When the tiered server determines that a local object file should be removed there are no writers to the file. This is because local object file removal can only be done on files that have become read-only. However, there may be active readers in the object file. At the time an object file is removed, the corresponding <code>WT_BLOCK</code> entry from the <code>handle_array</code> cannot be removed until all readers have finished using the block handle. The mechanism to remove a local object file has several parts.</p>
<p>When a cloud copy of an object is completed, the block manager is told, via <code>BM::switch_object_end</code> , that object id <em>N</em> (and therefore all ids less than <em>N</em>) can be removed. Then, when a block handle (<code>WT_BLOCK</code>) qualifies as removable and its reader reference count goes to zero, the block handle can be removed from the <code>handle_array</code>. This is triggered by calling a sweep function.</p>
<p>There is a potential for a race involving the read reference count: when the count becomes zero indicating the block can be removed, a new reader thread can enter the system and start using the block as it is being removed. To resolve the race, the reader thread takes a read lock on the block manager's lock while it increments the read reference count. When the sweep runs, it holds a write lock on the block manager's lock.</p>
<h1><a class="anchor" id="ts_server"></a>
Tiered Server and Work Queue</h1>
<p>Whether or not tiered storage is enabled, a <em>tiered server</em> thread is created. Its job is to process items on its work queue. Work items are actions that are generated by API calls that imply that some background action should take place. There is a single work queue, and items on it have a type, as follows:</p>
<ul>
<li><code>FLUSH:</code> copy a local object file to the cloud.</li>
<li><code>FLUSH_FINISH:</code> notify any local cloud caches that an object file has been copied to the cloud, and a local copy is still available to ingest.</li>
<li><code>REMOVE_LOCAL:</code> remove a local file that is has already been copied to the cloud.</li>
<li><code>REMOVE_SHARED:</code> remove a cloud file, perhaps as part of a drop operation.</li>
</ul>
<p>There are individual functions to add items of each type, and to get the next item, if any, of each type. That allows processing of items in a certain order. With only one server thread, all operations occur sequentially. It would make sense in the future to do <code>FLUSH</code> operations in parallel.</p>
<h1><a class="anchor" id="ts_metadata"></a>
Metadata and Associated Data Structures</h1>
<p>The WiredTiger metadata file persists important information about URIs known in the system. See <a class="el" href="arch-metadata.html#metadata_example">Example Metadata</a> for a discussion on the relationship between metadata entries and internal data structures for a simple non-tiered table <code>A</code>. That example is continued below for tiered tables.</p>
<p>For a tiered table <code>A</code> that has gone through a few flush_tiers, the following entries should appear in the metadata table:</p>
<ul>
<li><code>"table:A"</code> </li>
<li><code>"colgroup:A"</code> </li>
<li><code>"file:A-0000000004.wtobj"</code> </li>
<li><code>"object:A-0000000003.wtobj"</code> </li>
<li><code>"object:A-0000000002.wtobj"</code> </li>
<li><code>"object:A-0000000001.wtobj"</code> </li>
<li><code>"tier:A"</code> </li>
<li><code>"tiered:A"</code> </li>
</ul>
<p>The <code>"table:A"</code> entry, like before, is the top-level URI that is used for API calls. As before, it maps to a <code>WT_TABLE</code>, and the table has the <code>WT_TABLE-&gt;is_tiered_shared</code> flag set. The column group entry <code>"colgroup:A"</code> has in its metadata a reference to the URI for the btree. In the non-tiered case, this appeared as <code>source="file:A.wt"</code>. In this, the tiered case, the reference is <code>source="tiered:A"</code>, which relates to a <code>WT_TIERED</code> struct.</p>
<p>The <code>WT_TIERED</code> data structure has most of the useful information about a tiered table. In particular the btree is found in the dhandle. To be precise, <code>WT_TIERED-&gt;iface</code> is the dhandle, and <code>WT_TIERED-&gt;iface.handle</code> is a pointer to a WT_BTREE. The name of the current local object (<code>"file:A-0000000004.wtobj"</code> for this example) is stored in <code>WT_TIERED-&gt;tiers[WT_TIERED_INDEX_LOCAL]</code>. The URI of the btree (<code>"tiered:A"</code>) is kept in <code>WT_TIERED-&gt;tiers[WT_TIERED_INDEX_SHARED]</code>. Stored here is a pointer to a <code>WT_TIERED_TIERS</code> struct, this aligns with the <code>"tier:A"</code> entry in the metadata.</p>
<p>Also in the <code>WT_TIERED</code> struct is information about the current object id, and how many object ids are known. If the current writable object id is 4, the needed name can be constructed: <code>"file:A-0000000004.wtobj"</code>, likewise, previous cloud object names can be constructed, like <code>"object:A-0000000003.wtobj"</code>. Navigation like this, constructing names to look up in metadata or as data handles doesn't happen often, mostly during startup or flush_tier. During high performance paths, the <code>WT_TIERED</code> struct or the <code>WT_BM</code> (block manager struct) associated with the btree have everything needed.</p>
<p>A "tiered:" entry (associated with a tiered table) and a "file:" entry (associated with a non-tiered table) behave almost identically in the WiredTiger system. In fact, the <code>WT_BTREE_PREFIX</code> macro checks to see if a URI matches either one of these prefix strings. The macro basically means "does this thing walk and talk like a btree?". In both cases, the dhandle found with the given name has a <code>dhandle-&gt;handle</code> that points to the open <code>WT_BTREE</code>.</p>
<p>The <code>"object:"</code> entries do not have separate in-memory data structures (there is a <code>WT_TIERED_OBJECT</code> defined in <code>tiered.h</code>, but it is not used). And while <code>"table:"</code>, <code>"file:"</code> and <code>"tiered:"</code> all have data handles using those URIs, there are no separate data handles for each object. This is probably a good thing, as scaling the number of data handles system-wide has been challenging.</p>
<p>The following table summarizes the various relationships.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">URI prefix </th><th class="markdownTableHeadNone">struct type </th><th class="markdownTableHeadNone">has dhandle? </th><th class="markdownTableHeadNone">notes  </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>table:</code>  </td><td class="markdownTableBodyNone">WT_TABLE </td><td class="markdownTableBodyNone">yes </td><td class="markdownTableBodyNone">the dhandle is cast to <code></code>(WT_TABLE *)  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>file:</code>  </td><td class="markdownTableBodyNone">(none) </td><td class="markdownTableBodyNone">yes, but... </td><td class="markdownTableBodyNone">... does not relate to a btree  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>colgroup:</code>  </td><td class="markdownTableBodyNone">(none) </td><td class="markdownTableBodyNone">no </td><td class="markdownTableBodyNone">stored in an array in the WT_TABLE, references the "tiered:" entry  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>tiered:</code>  </td><td class="markdownTableBodyNone">WT_TIERED, WT_BTREE </td><td class="markdownTableBodyNone">yes </td><td class="markdownTableBodyNone">the dhandle is cast to <code></code>(WT_TIERED *), and dhandle-&gt;handle is a <code></code>(WT_BTREE *)  </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone"><code>tier:</code>  </td><td class="markdownTableBodyNone">WT_TIERED_TIERS </td><td class="markdownTableBodyNone">no </td><td class="markdownTableBodyNone">WT_TIERED-&gt;tiers[1] is a (WT_TIERED_TIERS *)  </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone"><code>object:</code>  </td><td class="markdownTableBodyNone">(none) </td><td class="markdownTableBodyNone">no </td><td class="markdownTableBodyNone">no dhandle for objects  </td></tr>
</table>
<h1><a class="anchor" id="ts_buckets"></a>
Storage Sources, Buckets, and Prefixes</h1>
<p>The location of objects in the cloud requires some information to be stored: the cloud provider, the bucket used, and an id to reference any needed credentials in a key management system. When a connection is opened for tiered storage, these are specified and are used in the default case. However, to have the ability to store tables in different clouds or buckets, cloud location information needs to live in the <code>WT_TIERED</code> object as well. To make this happen, all this <em>location</em> information is abstracted into a <code>WT_BUCKET_STORAGE</code> . The connection has a pointer to a <code>WT_BUCKET_STORAGE</code>, and each <code>WT_TIERED</code> does as well.</p>
<p>Among the fields of a <code>WT_BUCKET_STORAGE</code> is a pointer to a <code>WT_STORAGE_SOURCE</code>. The storage source can be thought of as a driver, or an abstraction of a cloud provider with operations. WiredTiger has several instances of <code>WT_STORAGE_SOURCE</code>, these include the drivers for the AWS, GCP, and Azure clouds, as well as <code>dir_store</code>, used for testing and development. A storage source can be asked to create a custom file system (returning a <code><a class="el" href="struct_w_t___f_i_l_e___s_y_s_t_e_m.html" title="The interface implemented by applications to provide a custom file system implementation.">WT_FILE_SYSTEM</a></code>) from a bucket name and credentials. A file system created this way is stored in the <code>WT_BUCKET_STORAGE</code>.</p>
<p>The file system can be used for various read-only operations, like listing the bucket contents or opening a <code><a class="el" href="struct_w_t___f_i_l_e___h_a_n_d_l_e.html" title="A file handle implementation returned by WT_FILE_SYSTEM::fs_open_file.">WT_FILE_HANDLE</a></code> . That, in turn, can be used to get the contents of a cloud object. Note that <code><a class="el" href="struct_w_t___f_i_l_e___s_y_s_t_e_m.html" title="The interface implemented by applications to provide a custom file system implementation.">WT_FILE_SYSTEM</a></code> and <code><a class="el" href="struct_w_t___f_i_l_e___h_a_n_d_l_e.html" title="A file handle implementation returned by WT_FILE_SYSTEM::fs_open_file.">WT_FILE_HANDLE</a></code> are more generic WiredTiger concepts, and are used outside of tiered storage.</p>
<p>The file system obtained from a storage source cannot be used to write pieces of data to objects, rather there is a method on the <code>WT_STORAGE_SOURCE</code> (cloud driver) that is used to copy a source object file in its entirety to the cloud. This makes it clear that objects must be written in their entirety, but may be read, if desired, in pieces.</p>
<p>In addition to the cloud providers, the storage source called <code>dir_store</code> emulates the behavior of a cloud provider, but stores objects in a directory. This is used for testing and development, avoiding the complication and expense of using cloud storage. By default, tests use a subdirectory of the WiredTiger home directory as the <code>dir_store</code> repository. Then, when a test breaks, the "cloud" objects are readily available for debugging.</p>
<p>We anticipate that buckets may be shared among multiple nodes, and possibly multiple clusters. To avoid name collisions, there is a prefix that can be given on a <a class="el" href="group__wt.html#gacbe8d118f978f5bfc8ccb4c77c9e8813" title="Open a connection to a database.">wiredtiger_open</a> call. It is expected that the caller gives WiredTiger a unique prefix. This prefix is stored as part of the <code>WT_BUCKET_STORAGE</code>, and so can also be specified per table upon creation. The prefix is prepended to every name stored in a bucket.</p>
<h1><a class="anchor" id="ts_flush"></a>
Flush Checkpoint Operations</h1>
<p>A flush_tier checkpoint has a slightly different sequence of operations from a regular checkpoint without flush_tier. In particular, eviction is only disabled when we are checkpointing the tree.</p>
<p>The first part of the checkpoint is determining which btrees need to participate. Following that, those btrees are checkpointed to disk. Finally, when the files have been written, they can be asynchronously copied (<em>flushed</em>) to cloud storage.</p>
<h2><a class="anchor" id="ts_checkpoint_prepare"></a>
Checkpoint Prepare Phase</h2>
<ul>
<li>Assemble a list of all the dhandles that will be part of the checkpoint. For non-tiered btrees this will be btrees that have been modified since the last checkpoint. For tiered btrees this will be btrees modified since the last flush. See <code>checkpoint_flush_tier</code> called from <code>checkpoint_prepare</code>.</li>
<li>For each tiered btree, create the new file, and queue a FLUSH work entry to flush the old one. The work entry includes checkpoint generation information that is used by the tiered server thread to know when it is okay to flush the file to object storage (i.e., after the current checkpoint has completed). See <code>tiered_switch</code>, which is called from <code>checkpoint_flush_tier</code>.</li>
<li>This work is done now so that it doesn't add time to the actual checkpoint when eviction is disabled.</li>
</ul>
<h2><a class="anchor" id="ts_checkpoint_data_file"></a>
Data file checkpoint</h2>
<p>Once the list of dhandles is known, each dhandle (that is, btree) is checkpointed:</p>
<ul>
<li>When doing the actual checkpoint for each btree, eviction to the btree is blocked. Hence checkpoint is the only thread updating the btree. This happens in <code>wt_sync_file</code>.</li>
<li>At the end of the checkpoint, after writing the new checkpoint root but before re-enabling eviction, the active file switches to be the new file created during the prepare phase. This ensures that everything that is part of the checkpoint is in the old file, and anything evicted after the checkpoint is in the next file.</li>
<li>If the btree is tiered, fsync is called on the old active file to ensure that all checkpoint updates are durable. Side note: the fsync doesn't have to be done here, it could happen after re-enabling eviction. But this describes the current actions of the code.</li>
<li>See <code>bm_checkpoint</code> for the active file switch and fsync.</li>
<li>After switching the file, eviction is allowed again as we finish <code>wt_sync_file</code>. Then the next dhandle is processed.</li>
</ul>
<h2><a class="anchor" id="ts_checkpoint_flush"></a>
Flush actions</h2>
<p>As indicated above, the FLUSH work entry is queued during the prepare phase and is processed asynchronously. The FLUSH entry indicates that WiredTiger should copy the completed file on disk to cloud storage.</p>
<p>When the FLUSH completes, these actions occur:</p><ul>
<li>tell the chunk cache to ingest the object before it is removed</li>
<li>update the metadata to reference the new object in the cloud</li>
<li>queue a FLUSH_FINISH operation</li>
<li>queue a REMOVE_LOCAL operation</li>
</ul>
<h1><a class="anchor" id="ts_future"></a>
Future</h1>
<p>There are some future features that are helpful to know about when studying the overall design.</p>
<h2><a class="anchor" id="ts_future_sharing"></a>
Sharing of Tiered Objects</h2>
<p>The current implementation of tiered storage supports btrees spanning objects that are stored locally and in cloud storage. Each cloud object is currently only useful and known to the system that created it. However, a larger design was in mind when the current implementation was made, and that informed a number of design decisions along the way.</p>
<p>The larger design allows all systems in a cluster to share knowledge about tiered objects. Generally, there is a single designated node in a cluster that calls flush_tier, this is the <em>flushing</em> node. Information about the objects stored as a result of the flush can be returned to the application, where it is transferred to other cooperating nodes in the cluster. These other nodes are known as "accepting" nodes, and accept this information, updating their metadata and incorporating references to the newly known objects. The mechanism for returning the flush information, and providing a way to incorporate the references, has not been implemented.</p>
<p>Another part of the sharing design is new kind of "union" table that is used to help incorporate new objects on the accepting nodes. The idea is that a tiered table has an additional layer. There is a local "tier", which is just a local btree, and a shared "tier" which is the tiered btree with multiple objects that has been described here. The cloud objects in the shared tier is shared among all the nodes of a replica set.</p>
<p>On both the flushing node and accepting nodes, any changes to a tiered table are inserted or updated into the local btree. Any lookups to the table consult the local btree first, then the shared btree. When a flush_tier is done on the flushing node, the set of changes up to a known timestamp <em>T</em> is moved from the local btree to the shared btree, a new object is created, and the previous object, which now contains the set changes, is pushed to the cloud. The accepting nodes receive the notification of the new cloud object, and the timestamp <em>T</em> associated with it. The accepting nodes trade out their own shared btree for a new shared btree rooted at the new cloud object. Any entries in an accepting node's local btree with a timestamp older than <em>T</em> are redundant as they must be included in the cloud object. There could be multiple strategies to remove the old entries.</p>
<p>The beauty of this design is that it is not difficult to understand (versus various alternatives), it does not require any downtime when new objects are accepted, nor any downtime if an accepting node is upgraded to a flushing node.</p>
<h2><a class="anchor" id="ts_future_garbage"></a>
Garbage Collection</h2>
<p>While there is a mechanism to create new objects, there is no removal, or <em>garbage collection</em> of objects that become redundant. That is, as new objects may completely cover sets of keys in the btree, pages having those keys in an older object are no longer needed. After all pages in an object are no longer needed, an object can be removed. The trick is in knowing when this can happen.</p>
<p>A future garbage collection solution could work well either synchronously or asynchronously. A synchronous approach would probably have WiredTiger track references to all pages in either a checkpoint or a object file (and persist that information as well), and notice when all references to an object have reached zero. This may require enhancements to extent lists in the block manager. An asynchronous approach could work mostly separately from WiredTiger (in another process possibly on a different node), and examine object files, visiting internal pages and tracing the references to all objects. As such, it can notice when an object file has no references. Either approach allows the identification and removal of unused objects, and maybe also identification of objects that are lightly filled. These objects could be made redundant by rewriting their useful content directly into the active btree. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="index.html">Reference Guide</a></li><li class="navelem"><a class="el" href="arch-index.html">WiredTiger Architecture Guide</a></li><li class="navelem"><a class="el" href="arch-toc-mem-disk.html">Moving Data Between Memory and Disk</a></li>
    <li class="footer">Copyright (c) 2008-present MongoDB, Inc.  All rights reserved.  Contact <a href="mailto:info@wiredtiger.com">info@wiredtiger.com</a> for more information.</li>
  </ul>
</div>
</body>
</html>
